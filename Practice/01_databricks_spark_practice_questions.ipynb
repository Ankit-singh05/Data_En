{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850ca9f5-ef47-49a6-9061-783d344f8ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Spark Practice - 30 Questions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains 30 comprehensive practice questions covering all major PySpark concepts. These questions are designed to be solved on Databricks and will help you master:\n",
    "\n",
    "- SparkSession and basic operations\n",
    "- Reading and writing data\n",
    "- DataFrame transformations\n",
    "- Aggregations and GroupBy\n",
    "- Spark SQL\n",
    "- Joins\n",
    "- Window functions\n",
    "- Complex data types\n",
    "- Performance optimization\n",
    "- Databricks-specific features\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **In Databricks**: SparkSession is automatically available as `spark`\n",
    "2. **For local testing**: Uncomment the SparkSession creation code in the setup cell\n",
    "3. Complete each exercise in the provided code cells\n",
    "4. Run the data setup cells first to create sample data\n",
    "5. Test your solutions by running the code and checking outputs\n",
    "6. Refer back to the PySpark module notebooks if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5c039e-5b49-462d-b3f0-88050deb2ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up all the sample data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3737be-9dd1-4b87-b85f-744db0e9c932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Databricks, SparkSession is already available\n",
    "# For local testing, uncomment the following:\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Databricks Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, ArrayType\n",
    "from pyspark.sql.functions import col, when, lit, expr, sum, avg, count, max, min, row_number, rank, dense_rank, lead, lag, window\n",
    "\n",
    "print(\"Setup complete! SparkSession ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842ab519-8070-455b-bf87-63eaddc91ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create employees DataFrame\n",
    "employees_data = [\n",
    "    (1, \"Alice\", 25, \"Sales\", 50000, \"NYC\", \"2020-01-15\"),\n",
    "    (2, \"Bob\", 30, \"IT\", 60000, \"LA\", \"2019-03-20\"),\n",
    "    (3, \"Charlie\", 35, \"Sales\", 70000, \"Chicago\", \"2018-06-10\"),\n",
    "    (4, \"Diana\", 28, \"IT\", 55000, \"NYC\", \"2021-02-14\"),\n",
    "    (5, \"Eve\", 32, \"HR\", 65000, \"Houston\", \"2019-11-05\"),\n",
    "    (6, \"Frank\", 27, \"Sales\", 52000, \"LA\", \"2022-01-08\"),\n",
    "    (7, \"Grace\", 29, \"IT\", 58000, \"Chicago\", \"2020-09-12\"),\n",
    "    (8, \"Henry\", 31, \"HR\", 62000, \"NYC\", \"2018-12-01\"),\n",
    "    (9, \"Ivy\", 26, \"Sales\", 51000, \"Houston\", \"2021-07-22\"),\n",
    "    (10, \"Jack\", 33, \"Finance\", 75000, \"LA\", \"2017-05-30\")\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(employees_data, employees_schema)\n",
    "print(\"Employees DataFrame created:\")\n",
    "df_employees.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5654ffc8-59cc-4b33-9d05-a81beb7ebf85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create departments DataFrame\n",
    "departments_data = [\n",
    "    (\"Sales\", \"John\", 1000000),\n",
    "    (\"IT\", \"Sarah\", 1500000),\n",
    "    (\"HR\", \"Mike\", 800000),\n",
    "    (\"Finance\", \"Lisa\", 1200000),\n",
    "    (\"Marketing\", \"Tom\", 900000)\n",
    "]\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"dept_name\", StringType(), True),\n",
    "    StructField(\"manager\", StringType(), True),\n",
    "    StructField(\"budget\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_departments = spark.createDataFrame(departments_data, departments_schema)\n",
    "print(\"Departments DataFrame created:\")\n",
    "df_departments.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb27637-ded1-4ed2-b153-0064441b9f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create sales DataFrame\n",
    "sales_data = [\n",
    "    (1, \"2024-01-15\", 1000, \"Product A\"),\n",
    "    (1, \"2024-02-20\", 1500, \"Product B\"),\n",
    "    (2, \"2024-01-10\", 2000, \"Product A\"),\n",
    "    (3, \"2024-02-05\", 1200, \"Product C\"),\n",
    "    (1, \"2024-03-12\", 1800, \"Product A\"),\n",
    "    (4, \"2024-01-25\", 900, \"Product B\"),\n",
    "    (2, \"2024-02-28\", 2200, \"Product C\"),\n",
    "    (5, \"2024-03-01\", 1100, \"Product A\"),\n",
    "    (3, \"2024-03-15\", 1300, \"Product B\")\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, sales_schema)\n",
    "print(\"Sales DataFrame created:\")\n",
    "df_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb91027-43c8-4252-8f5c-84c55167e27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create products DataFrame\n",
    "products_data = [\n",
    "    (\"Product A\", \"Electronics\", 500),\n",
    "    (\"Product B\", \"Clothing\", 300),\n",
    "    (\"Product C\", \"Electronics\", 800),\n",
    "    (\"Product D\", \"Food\", 50)\n",
    "]\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"base_price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_products = spark.createDataFrame(products_data, products_schema)\n",
    "print(\"Products DataFrame created:\")\n",
    "df_products.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67e9553-7303-4c5f-a7b3-270659a30f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Questions\n",
    "\n",
    "### Questions 1-5: Basic DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6270d8c-e110-4879-bf45-7619c794795b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 1: Filter and Select\n",
    "\n",
    "Filter `df_employees` to show only employees from the 'Sales' department, and select only the columns: `name`, `age`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3371fea8-5ce5-4273-8386-193eca36d083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(df_employees.filter(col(\"department\") == \"Sales\").select(\"name\", \"age\", \"salary\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349d89ec-71c8-413b-97cf-d3dbff8bc2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 2: Sort Data\n",
    "\n",
    "Sort `df_employees` by `salary` in descending order and show the top 5 employees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefeb76f-19e8-4521-ace5-db5405b4ccd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_employees.orderBy(col(\"salary\").desc()).limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7159a0-7011-4396-9dd4-caba0cb52fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 3: Add Calculated Column\n",
    "\n",
    "Add a new column `annual_bonus` to `df_employees` that is 10% of the salary. Display the result with columns: `name`, `salary`, and `annual_bonus`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d06c8cb-e2d9-4c01-bb3d-f45b7af1ea8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_employees.withColumn(\"annual_bonus\", col(\"salary\") * 0.10)\n",
    "    .select(\"name\", \"salary\", \"annual_bonus\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7595111-77a7-4646-b305-5602f95d6a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 4: Conditional Logic\n",
    "\n",
    "Create a new column `salary_category` in `df_employees` that categorizes salaries as:\n",
    "- \"High\" if salary >= 65000\n",
    "- \"Medium\" if salary >= 55000 and < 65000\n",
    "- \"Low\" if salary < 55000\n",
    "\n",
    "Show `name`, `salary`, and `salary_category`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c635ba-9830-4b38-bdca-4228a1133883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_employees.withColumn(\n",
    "        \"salary_category\",\n",
    "        when(col(\"salary\") >= 65000, \"High\")\n",
    "        .when(col(\"salary\") >= 55000, \"Medium\")\n",
    "        .otherwise(\"Low\")\n",
    "    ).select(\"name\", \"salary\", \"salary_category\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff093839-b942-4a04-8fa2-27b9ad824558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 5: Remove Duplicates and Null Handling\n",
    "\n",
    "Filter `df_employees` to remove any rows where `age` is null, then remove duplicate rows based on all columns. Count the total number of rows remaining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bd0e1a-9029-41e4-8694-b1509200b076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_df = df_employees.filter(col(\"age\").isNotNull()).dropDuplicates()\n",
    "row_count = filtered_df.count()\n",
    "row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91825b48-c0f3-496e-aa4f-39acb29c13a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 6-10: Aggregations and GroupBy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38fef39f-5381-4be4-93dc-210009c3b2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 6: Basic Aggregation\n",
    "\n",
    "Calculate the average salary for each department in `df_employees`. Show department and average salary, sorted by average salary in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ee778f-4feb-4c39-8ed7-68e98e9dc471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_employees.groupBy(\"department\")\n",
    "    .agg(avg(\"salary\").alias(\"average_salary\"))\n",
    "    .orderBy(col(\"average_salary\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1256e0-d20b-4c67-9b84-57be2139f4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 7: Multiple Aggregations\n",
    "\n",
    "For each department, calculate:\n",
    "- Total number of employees\n",
    "- Average salary\n",
    "- Maximum salary\n",
    "- Minimum salary\n",
    "\n",
    "Display the results sorted by department name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf88e340-1410-41de-a750-5f90ae56f03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_employees.groupBy(\"department\")\n",
    "    .agg(\n",
    "        count(\"emp_id\").alias(\"total_employees\"),\n",
    "        avg(\"salary\").alias(\"average_salary\"),\n",
    "        max(\"salary\").alias(\"max_salary\"),\n",
    "        min(\"salary\").alias(\"min_salary\")\n",
    "    )\n",
    "    .orderBy(\"department\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fef900c-5731-40e7-9ef9-cea0d43067df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 8: GroupBy with Filter\n",
    "\n",
    "Find the total sales amount (`amount`) for each employee (`emp_id`) in `df_sales`, but only include employees who have total sales greater than 2000. Show `emp_id` and total sales amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d92b869-53bc-46b3-9177-9e069f95f7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_sales.groupBy(\"emp_id\")\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\"))\n",
    "    .filter(col(\"total_sales\") > 2000)\n",
    "    .select(\"emp_id\", \"total_sales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5fd197-92ee-4714-88a4-aeff38c1762f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 9: Count Distinct\n",
    "\n",
    "Count the number of distinct cities where employees work in `df_employees`. Also, for each city, count how many employees work there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354e4f3f-4479-41f7-98a4-9e7604185d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "distinct_city_count = df_employees.select(\"city\").distinct().count()\n",
    "print(\"Number of distinct cities:\", distinct_city_count)\n",
    "\n",
    "display(\n",
    "    df_employees.groupBy(\"city\")\n",
    "    .agg(count(\"emp_id\").alias(\"employee_count\"))\n",
    "    .orderBy(\"city\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709c8197-c786-4bad-ad55-5e4a324481c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 10: Aggregation with Conditions\n",
    "\n",
    "Calculate the average age of employees for each department, but only include employees who are 30 years or older in the calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6515f3d-0fb1-4efd-a230-7a95c389ac16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df_employees.filter(col(\"age\") >= 30)\n",
    "    .groupBy(\"department\")\n",
    "    .agg(avg(\"age\").alias(\"average_age_30plus\"))\n",
    "    .orderBy(\"department\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9e09b9-992b-466b-bdab-b5c96672dcd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 11-15: Spark SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2016bbd5-7270-4a39-8397-e8a655af7ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 11: Create Temporary View and Query\n",
    "\n",
    "Create a temporary view from `df_employees` called `employees_view` and write a SQL query to find all employees in the 'IT' department with salary greater than 55000. Show `name`, `age`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70a624d-fb16-4f0f-b9fb-defb5ce6baa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT name, age, salary\n",
    "    FROM employees_view\n",
    "    WHERE department = 'IT' AND salary > 55000\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf373da9-596d-476c-8781-13a0627d779b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 12: SQL Aggregation\n",
    "\n",
    "Using Spark SQL, write a query to find the department with the highest total salary. Show the department name and total salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0d92e5-f12c-4ff9-bee7-20cccba79f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT department, SUM(salary) AS total_salary\n",
    "    FROM employees_view\n",
    "    GROUP BY department\n",
    "    ORDER BY total_salary DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259a9720-116f-4ef2-9874-3e108fc6cd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 13: SQL with CASE Statement\n",
    "\n",
    "Using Spark SQL, create a query that shows `name`, `salary`, and a new column `salary_band`:\n",
    "- 'A' for salary >= 70000\n",
    "- 'B' for salary >= 60000 and < 70000\n",
    "- 'C' for salary < 60000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c8a7d9-03ec-48d7-83a3-4c5af0a044ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        name,\n",
    "        salary,\n",
    "        CASE\n",
    "            WHEN salary >= 70000 THEN 'A'\n",
    "            WHEN salary >= 60000 AND salary < 70000 THEN 'B'\n",
    "            ELSE 'C'\n",
    "        END AS salary_band\n",
    "    FROM employees_view\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a31b71-85ae-4ee2-811c-0b9b8847b009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 14: SQL Subquery\n",
    "\n",
    "Using Spark SQL, find all employees whose salary is greater than the average salary of all employees. Show `name`, `department`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3907c796-6b5d-4699-8c36-12a16d16c029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees_view\n",
    "    WHERE salary > (SELECT AVG(salary) FROM employees_view)\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f844b1-a655-4b87-b76b-1b3070a85f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 15: SQL Window Function\n",
    "\n",
    "Using Spark SQL, rank employees within each department by their salary (highest salary gets rank 1). Show `name`, `department`, `salary`, and `rank`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067a368d-9f43-40e8-ade6-29d6534198a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS salary_rank\n",
    "    FROM employees_view\n",
    "\"\"\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f55aef-4f43-41fd-b64e-bef1b14363fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 16-20: Joins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91991eb0-200b-4d63-a4d1-6fb80fb27baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 16: Inner Join\n",
    "\n",
    "Perform an inner join between `df_employees` and `df_departments` on `department` = `dept_name`. Show `name`, `department`, `salary`, and `manager`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7418c5d9-ca19-4653-928c-6718705789fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"inner\"\n",
    ").select(\n",
    "    \"name\",\n",
    "    df_employees[\"department\"],\n",
    "    \"salary\",\n",
    "    \"manager\"\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b82d4b8-0fd6-488b-92b0-885f36900a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 17: Left Join\n",
    "\n",
    "Perform a left join between `df_employees` and `df_departments` on `department` = `dept_name`. This will show all employees even if their department doesn't exist in the departments table. Show `name`, `department`, and `manager`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ae59ef-bcfb-4c33-914c-bc42f48eb950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    \"name\",\n",
    "    df_employees[\"department\"],\n",
    "    \"manager\"\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da06a877-057d-49d1-b225-adfe78b521df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 18: Multiple Table Join\n",
    "\n",
    "Join `df_employees`, `df_sales`, and `df_products` to show:\n",
    "- Employee name\n",
    "- Sale date\n",
    "- Sale amount\n",
    "- Product name\n",
    "- Product category\n",
    "\n",
    "Use appropriate join types to include all sales records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1675ded-2cff-42c6-8ea3-d71eccaadb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_sales.join(\n",
    "    df_employees,\n",
    "    df_sales[\"emp_id\"] == df_employees[\"emp_id\"],\n",
    "    \"left\"\n",
    ").join(\n",
    "    df_products,\n",
    "    df_sales[\"product\"] == df_products[\"product_name\"],\n",
    "    \"left\"\n",
    ").select(\n",
    "    df_employees[\"name\"],\n",
    "    df_sales[\"sale_date\"],\n",
    "    df_sales[\"amount\"],\n",
    "    df_sales[\"product\"],\n",
    "    df_products[\"category\"]\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fd5bea-cd26-4ffc-9da7-a1eb2b82c408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 19: Left Semi Join\n",
    "\n",
    "Use a left semi join to find all employees from `df_employees` who have made at least one sale (exist in `df_sales`). Show only the employee information: `name`, `department`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f7df7b-c6b5-4eb0-b30e-b46734b0034d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_employees.join(\n",
    "    df_sales,\n",
    "    df_employees[\"emp_id\"] == df_sales[\"emp_id\"],\n",
    "    \"left_semi\"\n",
    ").select(\n",
    "    \"name\",\n",
    "    \"department\",\n",
    "    \"salary\"\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437f3024-9d09-4d4d-9a47-280a0b961450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 20: Anti Join\n",
    "\n",
    "Use an anti join to find all employees from `df_employees` who have NOT made any sales (do not exist in `df_sales`). Show `name`, `department`, and `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331c4e60-1b91-495f-9b7a-bdddd3540cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_employees.join(\n",
    "    df_sales,\n",
    "    df_employees[\"emp_id\"] == df_sales[\"emp_id\"],\n",
    "    \"left_anti\"\n",
    ").select(\n",
    "    \"name\",\n",
    "    \"department\",\n",
    "    \"salary\"\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560b75cc-d8e2-4b14-a1b3-cf6cee39e124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 21-25: Window Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9850fe8a-d1bb-4690-a1e1-7fe81299113b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 21: Row Number\n",
    "\n",
    "Use a window function to assign row numbers to employees within each department, ordered by salary in descending order. Show `name`, `department`, `salary`, and `row_number`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc75133-6141-4896-83b0-c310b400ea13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "result_df = df_employees.withColumn(\n",
    "    \"row_number\",\n",
    "    row_number().over(window_spec)\n",
    ").select(\"name\", \"department\", \"salary\", \"row_number\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04dd6b3d-d6a9-4835-8aa4-f7a260fd4318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 22: Rank and Dense Rank\n",
    "\n",
    "Calculate both `rank` and `dense_rank` for employees within each department based on salary. Show `name`, `department`, `salary`, `rank`, and `dense_rank`. Notice the difference between rank and dense_rank when there are ties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b751f2-6c77-4676-a2e7-dd154d434bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "result_df = df_employees.withColumn(\n",
    "    \"rank\", rank().over(window_spec)\n",
    ").withColumn(\n",
    "    \"dense_rank\", dense_rank().over(window_spec)\n",
    ").select(\"name\", \"department\", \"salary\", \"rank\", \"dense_rank\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363644eb-8f60-47c2-b1af-2872fc5e9988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 23: Running Total\n",
    "\n",
    "Calculate a running total of sales amounts for each employee in `df_sales`, ordered by `sale_date`. Show `emp_id`, `sale_date`, `amount`, and `running_total`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5f6467-77d0-4d94-9a27-3171842bf2dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "result_df = df_sales.withColumn(\n",
    "    \"running_total\",\n",
    "    sum(\"amount\").over(window_spec)\n",
    ").select(\"emp_id\", \"sale_date\", \"amount\", \"running_total\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c8a4c3-3c22-46e9-876b-53f83aa4e1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 24: Lead and Lag\n",
    "\n",
    "For each employee's sales in `df_sales`, show:\n",
    "- Current sale amount\n",
    "- Previous sale amount (lag)\n",
    "- Next sale amount (lead)\n",
    "\n",
    "Order by `emp_id` and `sale_date`. Show `emp_id`, `sale_date`, `amount`, `prev_amount`, and `next_amount`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e86a98-c9dc-45bd-8346-b7e0ee056b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\").orderBy(\"sale_date\")\n",
    "\n",
    "result_df = df_sales.withColumn(\n",
    "    \"prev_amount\", lag(\"amount\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"next_amount\", lead(\"amount\", 1).over(window_spec)\n",
    ").select(\n",
    "    \"emp_id\", \"sale_date\", \"amount\", \"prev_amount\", \"next_amount\"\n",
    ").orderBy(\"emp_id\", \"sale_date\")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81576e90-319d-4a5e-8173-0588df2ed0b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 25: Window Aggregation\n",
    "\n",
    "For each sale in `df_sales`, calculate:\n",
    "- Average sale amount for the same employee\n",
    "- Maximum sale amount for the same employee\n",
    "- Minimum sale amount for the same employee\n",
    "\n",
    "Show `emp_id`, `sale_date`, `amount`, `avg_amount`, `max_amount`, and `min_amount`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1323185-730b-4006-a37e-7770f5df9633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, max, min\n",
    "\n",
    "window_spec = Window.partitionBy(\"emp_id\")\n",
    "\n",
    "result_df = df_sales.withColumn(\n",
    "    \"avg_amount\", avg(\"amount\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"max_amount\", max(\"amount\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"min_amount\", min(\"amount\").over(window_spec)\n",
    ").select(\n",
    "    \"emp_id\", \"sale_date\", \"amount\", \"avg_amount\", \"max_amount\", \"min_amount\"\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29a54e4-05b9-402b-809f-4aa5543ac6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Questions 26-30: Advanced Topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71476db4-020a-4bfa-af43-6e055fbc42b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 26: Pivot Operation\n",
    "\n",
    "Pivot `df_sales` to show total sales amount for each employee (`emp_id`) by product. The result should have columns: `emp_id`, `Product A`, `Product B`, `Product C` (and `Product D` if applicable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c814626c-cbed-4330-9abf-90b4000c02f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = df_sales.groupBy(\"emp_id\").pivot(\"product\", [\"Product A\", \"Product B\", \"Product C\", \"Product D\"]).sum(\"amount\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07126676-e25c-4c90-84ef-c684c0f9a279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 27: Union Operation\n",
    "\n",
    "Create two DataFrames:\n",
    "1. Employees from 'Sales' department\n",
    "2. Employees from 'IT' department\n",
    "\n",
    "Union them together and show the result with columns: `name`, `department`, `salary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8434b1-2206-4b1a-a0c1-dd736b5846f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales_dept = df_employees.filter(col(\"department\") == \"Sales\").select(\"name\", \"department\", \"salary\")\n",
    "df_it_dept = df_employees.filter(col(\"department\") == \"IT\").select(\"name\", \"department\", \"salary\")\n",
    "result_df = df_sales_dept.union(df_it_dept)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb7ad26-404d-4ba7-b80c-2501beaab284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 28: Complex Aggregation with Multiple Conditions\n",
    "\n",
    "For each department in `df_employees`, calculate:\n",
    "- Total number of employees\n",
    "- Number of employees with salary > 60000\n",
    "- Average salary for employees with salary > 60000\n",
    "- Average salary for all employees\n",
    "\n",
    "Show all results in a single query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428a7f86-79a2-4c17-9580-647764e223fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg, sum, when\n",
    "\n",
    "result_df = df_employees.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"total_employees\"),\n",
    "    sum(when(col(\"salary\") > 60000, 1).otherwise(0)).alias(\"employees_salary_gt_60000\"),\n",
    "    avg(when(col(\"salary\") > 60000, col(\"salary\"))).alias(\"avg_salary_gt_60000\"),\n",
    "    avg(\"salary\").alias(\"avg_salary_all\")\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb1774b-c029-4a47-86ae-646d8241c2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 29: Reading and Writing Data (Databricks)\n",
    "\n",
    "**In Databricks:**\n",
    "1. Write `df_employees` to a Parquet file in Volumes at path `Volumes/workspace/default/databricks_practice/employees/`\n",
    "2. Read the data back from that path into a new DataFrame\n",
    "3. Verify by showing the first 5 rows\n",
    "\n",
    "**Note:** \n",
    "- In Databricks, use the Volumes path format: `Volumes/workspace/default/<catalog_name>/<schema_name>/<path>`\n",
    "- For local testing, use a local path like `./data/output/employees/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0866743f-186c-4d57-915a-6cbbc2146807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For Databricks (Volumes path):\n",
    "output_path = \"/Volumes/demo_catalog/demo_schema/demo_volume/sellers_dataset.csv\"\n",
    "\n",
    "df_employees.write.mode(\"overwrite\").csv(output_path)\n",
    "df_employees_parquet = spark.read.csv(output_path)\n",
    "display(df_employees_parquet.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f85aa1-87f0-425c-afe7-158d3d99b377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 30: Complete ETL Pipeline\n",
    "\n",
    "Create a complete ETL pipeline that:\n",
    "1. **Extract**: Join `df_employees` and `df_sales` to get employee sales data\n",
    "2. **Transform**: \n",
    "   - Calculate total sales per employee\n",
    "   - Add a column `performance` that is \"Excellent\" if total sales > 3000, \"Good\" if > 2000, else \"Average\"\n",
    "   - Join with `df_employees` to get employee details\n",
    "3. **Load**: Select and display the final result with columns: `name`, `department`, `total_sales`, `performance`\n",
    "\n",
    "Chain all operations together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c967b8-fdeb-47d8-9997-91b157651cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, when, col\n",
    "\n",
    "result_df = (\n",
    "    df_employees.join(df_sales, \"emp_id\", \"inner\")\n",
    "    .groupBy(\"emp_id\")\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\"))\n",
    "    .withColumn(\n",
    "        \"performance\",\n",
    "        when(col(\"total_sales\") > 3000, \"Excellent\")\n",
    "        .when(col(\"total_sales\") > 2000, \"Good\")\n",
    "        .otherwise(\"Average\")\n",
    "    )\n",
    "    .join(df_employees.select(\"emp_id\", \"name\", \"department\"), \"emp_id\", \"inner\")\n",
    "    .select(\"name\", \"department\", \"total_sales\", \"performance\")\n",
    ")\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469db304-e076-4ff3-baa6-b6a0c9138c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Additional Challenges (Optional)\n",
    "\n",
    "If you've completed all 30 questions, try these advanced challenges:\n",
    "\n",
    "1. **Performance Optimization**: Repartition `df_employees` by `department` and cache it. Measure the performance improvement.\n",
    "\n",
    "2. **Complex Window Function**: Calculate the 3-month moving average of sales for each employee.\n",
    "\n",
    "3. **Broadcast Join**: Use broadcast join hint for joining `df_employees` with `df_departments` (assuming departments is small).\n",
    "\n",
    "4. **Date Operations**: Convert `hire_date` in `df_employees` to DateType and calculate the number of years each employee has been with the company.\n",
    "\n",
    "5. **Array Operations**: Create an array column containing all cities where each department has employees, then explode it.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing the practice questions! These exercises covered:\n",
    "\n",
    " Basic DataFrame operations (filter, select, sort)\n",
    "\n",
    " Aggregations and GroupBy\n",
    "\n",
    " Spark SQL queries\n",
    "\n",
    " Various join types\n",
    "\n",
    " Window functions\n",
    "\n",
    " Advanced transformations\n",
    "\n",
    " ETL pipeline creation\n",
    "\n",
    "Keep practicing and refer back to the PySpark module notebooks for detailed explanations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f841cf3-999d-48a3-8995-78739131975e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_databricks_spark_practice_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
